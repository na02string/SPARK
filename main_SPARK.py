import os
import sys
import random
from time import time

import pandas as pd
from tqdm import tqdm
import torch.nn as nn
import torch.optim as optim
from prompts.prompt_SPARK import aspect_SPARK_Prompt
from utils.aspect_extractor import extract_user_aspects
from utils.beamsearch import CollaborativeBeamSearchWithAspect 
from utils.node_aspect_mapper import NodeAspectMapper
from utils.utils import llm_response

from utils.evidence_builder import build_kg_evidence
from utils.opinion_extractor import extract_item_opinion
import json

# ì‚¬ìš©ì ë§ì¶¤ ì¶”ì²œ ì„¤ëª… ìƒì„± íŒŒì´í”„ë¼ì¸
#aspect-aware personalized reasoning + explanationì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” í•¨ìˆ˜
def get_SPARK_result(args, model, data, uid, iid):
    """
    args : íŒŒë¼ë¯¸í„° ì„¤ì • í¬í•¨ (hops, num_beams, num_paths ë“±)
    model : í•™ìŠµëœ KGAT ëª¨ë¸
    data : DataLoaderKGAT ê°ì²´
    uid, iid : ì‚¬ìš©ì ë° ì¶”ì²œ ì•„ì´í…œ
    """
    # user uid ì™€ ì•„ì´í…œ iidì— ëŒ€í•´ ì¶”ì²œ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜. 

    # 1ë‹¨ê³„: ì‚¬ìš©ì ë¦¬ë·° ê¸°ë°˜ ì„ í˜¸ Aspect ì¶”ì¶œ
    aspect_score_dict = extract_user_aspects(args, data, uid)
    # ğŸ†•  NodeAspectMapper ìºì‹œ ê°ì²´
    node_mapper = NodeAspectMapper(cache_path="cache/node_aspect_cache.jsonl")

    # 2ë‹¨ê³„: BeamSearch ê°ì²´ ìƒì„± (aspect score + mapper + Î» ë°˜ì˜)
    BeamSearch = CollaborativeBeamSearchWithAspect(
                    data=data,
                    model=model,
                    user_aspect_dict=aspect_score_dict,
                    node_mapper=node_mapper,
                    lambda_aspect=args.lambda_aspect   # parserì— ì¶”ê°€í•´ë‘” ê°’
                    ,
                    args = args
                )

    # 3ë‹¨ê³„: ì¶”ì²œ ê²½ë¡œ íƒìƒ‰
    save_path = {}
    
    # ë‹¤ì–‘í•œ hop ìˆ˜ ì— ëŒ€í•´ ì¶”ì²œ ê²½ë¡œ ì¶”ì¶œ. -> 3,5 ë¡œ ì„¤ì •í•´ë‘ 
    for hops in args.hops:
        # valid_paths : ì˜ë¯¸ ìˆëŠ” ê²½ë¡œ
        valid_paths, paths = BeamSearch.search(uid, iid, 
                          only_attribute=True, remove_duplicate=True, 
                          num_beams=args.num_beams, num_hops=hops)
        
        if len(valid_paths) == 0:
            print(f"[SKIP] No valid path found for {hops} hop â€” skipping this hop.")
            continue        
        
        print(f"{hops} hop / valid_paths: {len(valid_paths)}, total_paths: {len(paths)}")
        save_path[hops] = BeamSearch.path2linearlize(valid_paths, to_original_name=True)
        # path2linearlize : ì¶”ì¶œí•œ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë°”ê¿ˆ. => LLM ì…ë ¥ìš©
# hopë“¤ì„ ëª¨ë‘ íƒìƒ‰í•œ í›„, ì–´ë–¤ hopì—ì„œë„ valid pathê°€ ì—†ìœ¼ë©´ ì„¤ëª… ìŠ¤í‚µ
    if not save_path:
        print("[SKIP] No valid paths found for any hop â€” skipping explanation.")
        return None
    
    print(f"[DEBUG] save_path : {save_path}")
    
    # 4ë‹¨ê³„: ê²½ë¡œ ì„ íƒ ë° ìš”ì•½    
    selected_path = []
    for hops, paths in save_path.items():
        # hop ë³„ë¡œ ìƒìœ„ num_pathsê°œì˜ ê²½ë¡œë§Œ ì„ íƒ. ë””í´íŠ¸ë¡œ 4ê°œë¡œ í•´ë‘ 
        selected_path.extend([path[0] for path in paths[:args.num_paths]])
        # ['A2B73CL3QSYWLB -> user_likes_item -> 1920s_rediscovered_films -> attribute_is_subject_of_item: -> The_Beloved_Rogue -> item_has_subject_as_attribute: -> 1920s_historical_adventure_films',
        #  'A2B73CL3QSYWLB -> user_likes_item -> 1927_films -> attribute_is_subject_of_item: -> The_Beloved_Rogue -> item_has_subject_as_attribute: -> 1920s_historical_adventure_films']
    selected_path_str = '\n'.join(selected_path)

    # 5ë‹¨ê³„: LLM ê¸°ë°˜ IC ìš”ì•½ ìƒì„±    
    # ì²«ë²ˆì§¸ LLM ì…ë ¥ ìƒì„±(IC ìš”ì•½ ìš”ì²­)
    item_information=BeamSearch.item_information(iid,max_relations=5) # itemì— ëŒ€í•œ ê¸°ë³¸ ì •ë³´ë‘
    user_history = BeamSearch.user_history(uid, max_items=5, max_lines=10) # userê°€ ì´ì „ì— ë³¸ item ìš”ì•½(user_history)ë¥¼ ë¶ˆëŸ¬ì™€ì„œ 
    # === 6 ë‹¨ê³„: KG evidence ìƒì„± ===
    kg_evi = build_kg_evidence(
                args=args,
                paths=selected_path,   # ê²½ë¡œ ë¬¸ìì—´ë§Œ
                aspect_dict=aspect_score_dict,
                data=data,
                node_mapper=node_mapper
            )

    # === 7 ë‹¨ê³„: item opinion ì¶”ì¶œ ===
    opinions = []
    ic_json = []
    for asp, evid_list in kg_evi.items():
        if not evid_list:  # evidenceê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            continue
        opinions = extract_item_opinion(args, data, iid, asp)
        ic_json.append({
            "aspect": asp,
            "aspect_score" : aspect_score_dict.get(asp,0.0),
            "kg_evidence": evid_list[:10],   # ìµœëŒ€ 10ê°œ
            "item_opinion": opinions
        })
        ic_json.sort(key=lambda x: -x["aspect_score"]) # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬

    # === 5â€‘ë‹¨ê³„: LLM ì„¤ëª… ìƒì„± ===
    IC2explanation_formatted = aspect_SPARK_Prompt.aspect_explanation_prompt.format(
                                    context=json.dumps(ic_json, ensure_ascii=False, indent=2),
                                    user=data.user_id2org[uid],
                                    item=data.entity_id2org[iid],
                                    record=user_history,
                                    item_information=item_information)
    print(f'[DEBUG] user_history {user_history}')
    print(f"[DEBUG] item_information {item_information}")
    print(f"[DEBUG] ic_json : {json.dumps(ic_json, ensure_ascii=False, indent=2)}")
    print(f"[DEBUG]kg_ev : {evid_list[:10]}")
    print(f"[DEBUG]item opinion : {opinions}")
    explanation = llm_response(args, IC2explanation_formatted)
    print(f"[DEBUG] Prompt : {IC2explanation_formatted}")
    # ë¡œê·¸ ê¸°ë¡ ì¶”ê°€
    import logging
    logging.info(f"[PROMPT] LLM Input Prompt for uid={uid}, iid={iid}:\n{IC2explanation_formatted}")

    print(f"[DEBUG] LLM Output: {explanation}")
    # ìµœì¢… ì¶”ì²œ ì„¤ëª… ë°˜í™˜
    return explanation
